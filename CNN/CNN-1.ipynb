{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPik4DXg+Sn1VpH8qNdNQjP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jexCW6E5bGVR"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Explanation\n","1. Import necessary libraries.\n","2. Load your dataset (here, MNIST).\n","3. Preprocess data by reshaping and normalizing.\n","4. Split data into training, validation, and test sets.\n","5. Define CNN model architecture using Keras' Sequential API.\n","6. Compile model with optimizer, loss function, and metrics.\n","7. Train model using fit() method.\n","8. Evaluate model on test data using evaluate() method.\n","9. Use model for predictions using predict() method.\n","#Model Architecture\n","###This CNN consists of:\n","1. Conv2D layer with 32 filters, kernel size 3x3, and ReLU activation.\n","MaxPooling2D layer with pool size 2x2.\n","2. Flatten layer.\n","3. Dense layer with 64 units, ReLU activation, and dropout (20%).\n","4. Output Dense layer with 10 units (for MNIST's 10 classes) and softmax activation.\n","#Advice\n","1. Experiment with different architectures, hyperparameters, and optimizers.\n","2. Use techniques like data augmentation, transfer learning, or batch normalization to improve performance.\n","3. Monitor training and validation accuracy to avoid overfitting."],"metadata":{"id":"cRlik-aCbJ4c"}},{"cell_type":"code","source":["# Define CNN model architecture\n","model = keras.Sequential([\n","keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n","keras.layers.MaxPooling2D((2, 2)),\n","keras.layers.Flatten(),\n","keras.layers.Dense(64, activation='relu'),\n","keras.layers.Dropout(0.2),\n","keras.layers.Dense(10, activation='softmax')\n","])"],"metadata":{"id":"Kg-7uPa9b4Wp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Here's a breakdown of the CNN model architecture:\n","#**Layer 1: Conv2D (Convolutional Layer)**\n","##keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))\n","###Parameters:\n","1. 32: Number of filters (also known as feature detectors or kernels). Each filter will learn to detect a specific feature in the input image.\n","2. (3, 3): Filter size (or kernel size). The filter will scan the input image in 3x3 patches.\n","3. activation='relu': Activation function used to introduce non-linearity. ReLU (Rectified Linear Unit) outputs 0 for negative inputs and the input value for positive inputs.\n","4. input_shape=(28, 28, 1): Input shape of the data. In this case, 28x28 grayscale images (1 color channel).\n","####Neurons: 32 feature maps, each with 26x26 neurons (due to 3x3 filter and 1 padding).\n","#**Layer 2: MaxPooling2D (Pooling Layer)**\n","###keras.layers.MaxPooling2D((2, 2))\n","###Parameters:\n","1. (2, 2): Pooling size. The layer will reduce spatial dimensions by taking the maximum value across 2x2 patches.\n","2. Neurons: 32 feature maps, each with 13x13 neurons (due to 2x2 pooling).\n","#**Layer 3: Flatten**\n","###keras.layers.Flatten()\n","1. Purpose: Flatten the feature maps into a 1D vector to prepare for fully connected layers.\n","2. Neurons: 32 * 13 * 13 = 5,408 neurons ( flattening the 32 feature maps).\n","#**Layer 4: Dense (Fully Connected Layer)**\n","###keras.layers.Dense(64, activation='relu')\n","###Parameters:\n","1. 64: Number of neurons in the layer.\n","2. activation='relu': Activation function used to introduce non-linearity.\n","3. Neurons: 64 neurons.\n","#**Layer 5: Dropout**\n","###keras.layers.Dropout(0.2)\n","###Parameters:\n","1. 0.2: Dropout rate. Randomly sets 20% of neurons to zero during training.\n","2. Purpose: Regularization technique to prevent overfitting.\n","#**Layer 6: Dense (Output Layer)**\n","###keras.layers.Dense(10, activation='softmax')\n","###Parameters:\n","1. 10: Number of neurons in the layer (one for each class).\n","2. activation='softmax': Activation function used to output probabilities.\n","3. Neurons: 10 neurons (output probabilities for each class).\n","#**Model Summary:**\n","1. Total parameters: approximately 1,218,706\n","2. Total neurons: 32 * 26 * 26 + 32 * 13 * 13 + 5,408 + 64 + 10 = 61,934\n","3. This architecture is a simple CNN designed for handwritten digit recognition (MNIST dataset). You may need to adjust the architecture based on your specific problem and dataset."],"metadata":{"id":"0CXOvYRkbvao"}},{"cell_type":"markdown","source":["###Conv2D and MaxPooling2D are two fundamental layers in Convolutional Neural Networks (CNNs). Here's a comparison:\n","#**Conv2D (Convolutional Layer)**\n","1. Purpose: Feature extraction\n","2. Function: Scans input images using filters (kernels) to detect local  patterns and features\n","3. Parameters:\n","\n","  Filter size (kernel size)\n","\n","  Number of filters\n","\n","  Stride\n","\n","  Padding\n","\n","4. Output: Feature maps (activated feature detectors)\n","##**MaxPooling2D (Pooling Layer)**\n","1. Purpose: Spatial downsampling\n","2. Function: Reduces spatial dimensions by retaining maximum values across patches\n","3. Parameters:\n","  Pooling size\n","  Stride\n","  Padding\n","4. Output: Downsampled feature maps\n","##**Key differences:**\n","1. Purpose: Conv2D extracts features, while MaxPooling2D reduces spatial dimensions.\n","2. Filter size: Conv2D uses smaller filters (e.g., 3x3) to detect local features, whereas MaxPooling2D uses larger pooling sizes (e.g., 2x2) to downsample.\n","3. Output: Conv2D produces feature maps with the same spatial dimensions as the input, while MaxPooling2D reduces spatial dimensions.\n","4. Information retention: Conv2D retains all information from the input, whereas MaxPooling2D discards information by taking only the maximum value.\n","##**When to use:**\n","###**Conv2D:**\n","1. Initial layers to extract low-level features\n","2. When preserving spatial information is crucial\n","##**MaxPooling2D:**\n","1. After Conv2D layers to reduce spatial dimensions and retain important features\n","2. To decrease computational complexity and number of parameters"],"metadata":{"id":"NnArOO69hqp8"}},{"cell_type":"markdown","source":["###In the context of images and convolutional neural networks (CNNs), spatial dimensions refer to the:\n","##**Width (W) and Height (H)**\n","of an image or feature map.\n","##**Spatial dimensions represent:**\n","1. Image size: The number of pixels in the width and height of an image.\n","2. Feature map size: The number of neurons in the width and height of a feature map.\n","##**Reducing spatial dimensions:**\n","1. Means decreasing the width and height of an image or feature map, while retaining the most important information.\n","##**Why reduce spatial dimensions?:**\n","1. Computational efficiency: Fewer pixels/neurons require less computation.\n","2. Information condensation: Retains essential features while discarding redundant information.\n","3. Improved generalization: Helps prevent overfitting by reducing the model's capacity.\n","##Examples of spatial dimensions:\n","1. Image: 28x28 (width x height)\n","2. Feature map: 26x26 (after convolution)\n","3. Pooled feature map: 13x13 (after max pooling)\n","##Common spatial dimension reductions:\n","1. Max Pooling: Reduces spatial dimensions by taking maximum values across patches.\n","2. Average Pooling: Reduces spatial dimensions by taking average values across patches.\n","3. Stride: Increases the step size between pixels/neurons during convolution or pooling.\n","\n","By reducing spatial dimensions, CNNs can efficiently process and analyze images while retaining essential features."],"metadata":{"id":"6-Uqp9S9jmNE"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load MNIST dataset (replace with your own dataset)\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","\n","# Preprocess data\n","x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n","x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n","\n","# Split data into training and validation sets\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n","\n","# Define CNN model architecture\n","model = keras.Sequential([\n","    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n","    keras.layers.MaxPooling2D((2, 2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train model\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    validation_data=(x_val, y_val),\n","                    batch_size=128)\n","\n","# Evaluate model on test data\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","print(f'Test accuracy: {test_acc:.2f}')\n","\n","# Use model for predictions\n","predictions = model.predict(x_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6qO5XQcbKOz","executionInfo":{"status":"ok","timestamp":1727425688176,"user_tz":-330,"elapsed":413209,"user":{"displayName":"sdebbcbsnl rkl","userId":"03090754050651513501"}},"outputId":"a2ab5fbc-f63a-474b-8442-96e05012113e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 61ms/step - accuracy: 0.7971 - loss: 0.6689 - val_accuracy: 0.9597 - val_loss: 0.1344\n","Epoch 2/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.9540 - loss: 0.1570 - val_accuracy: 0.9741 - val_loss: 0.0914\n","Epoch 3/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 62ms/step - accuracy: 0.9717 - loss: 0.0975 - val_accuracy: 0.9790 - val_loss: 0.0710\n","Epoch 4/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9774 - loss: 0.0756 - val_accuracy: 0.9823 - val_loss: 0.0592\n","Epoch 5/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - accuracy: 0.9814 - loss: 0.0613 - val_accuracy: 0.9833 - val_loss: 0.0564\n","Epoch 6/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 61ms/step - accuracy: 0.9835 - loss: 0.0552 - val_accuracy: 0.9832 - val_loss: 0.0553\n","Epoch 7/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.9851 - loss: 0.0480 - val_accuracy: 0.9851 - val_loss: 0.0501\n","Epoch 8/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 55ms/step - accuracy: 0.9869 - loss: 0.0415 - val_accuracy: 0.9857 - val_loss: 0.0484\n","Epoch 9/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 58ms/step - accuracy: 0.9884 - loss: 0.0366 - val_accuracy: 0.9856 - val_loss: 0.0494\n","Epoch 10/10\n","\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0313 - val_accuracy: 0.9857 - val_loss: 0.0491\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9819 - loss: 0.0539\n","Test accuracy: 0.99\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n"]}]},{"cell_type":"code","source":["x_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QCCOpo8mMI8","executionInfo":{"status":"ok","timestamp":1727425770142,"user_tz":-330,"elapsed":760,"user":{"displayName":"sdebbcbsnl rkl","userId":"03090754050651513501"}},"outputId":"b8246941-4cce-4b9f-efbf-265831b6d41b"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 28, 28, 1)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UtP22kPzmbvi","executionInfo":{"status":"ok","timestamp":1727425797953,"user_tz":-330,"elapsed":594,"user":{"displayName":"sdebbcbsnl rkl","userId":"03090754050651513501"}},"outputId":"cf1fa288-84a8-42ad-b22b-bce71966c395"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[7.6600509e-10, 4.2806899e-08, 1.4486611e-07, ..., 9.9999577e-01,\n","        6.6329015e-10, 1.8260446e-06],\n","       [2.8832571e-07, 2.8635561e-04, 9.9970675e-01, ..., 5.4813935e-11,\n","        3.9305930e-11, 1.9705484e-13],\n","       [2.1803858e-07, 9.9997205e-01, 1.2770590e-06, ..., 8.0216123e-06,\n","        1.1853961e-06, 1.1637455e-07],\n","       ...,\n","       [1.2841440e-12, 9.9677278e-10, 3.5145287e-12, ..., 2.5062976e-09,\n","        5.9284155e-09, 5.7497510e-07],\n","       [6.6009184e-11, 1.0434961e-09, 3.1113426e-13, ..., 3.2526804e-11,\n","        2.5083074e-05, 4.4187078e-11],\n","       [1.3043525e-07, 4.9983083e-08, 1.7724524e-08, ..., 1.4853310e-10,\n","        1.3930564e-08, 3.6260786e-12]], dtype=float32)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Make predictions on unseen test data\n","y_pred = model.predict(x_test)\n","y_pred_class = np.argmax(y_pred, axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzu5kcRim0Uk","executionInfo":{"status":"ok","timestamp":1727425909904,"user_tz":-330,"elapsed":2402,"user":{"displayName":"sdebbcbsnl rkl","userId":"03090754050651513501"}},"outputId":"970bf8ad-c933-40a8-d21e-fbc1921bc8cc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n"]}]},{"cell_type":"code","source":["y_pred_class"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyOraI5Um58M","executionInfo":{"status":"ok","timestamp":1727425919439,"user_tz":-330,"elapsed":719,"user":{"displayName":"sdebbcbsnl rkl","userId":"03090754050651513501"}},"outputId":"253fae6b-6ba9-47e3-e77e-5806d58d9369"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7, 2, 1, ..., 4, 5, 6])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Evaluate model performance using accuracy score\n","accuracy = accuracy_score(y_test, y_pred_class)\n","print(f'Test Accuracy (sklearn): {accuracy:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Fgc5EWPm0XL","executionInfo":{"status":"ok","timestamp":1727425929049,"user_tz":-330,"elapsed":588,"user":{"displayName":"sdebbcbsnl rkl","userId":"03090754050651513501"}},"outputId":"fb95e6e2-e135-432e-e8fb-e8f7be50be39"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy (sklearn): 0.99\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0Dvizx5Am0Z1"},"execution_count":null,"outputs":[]}]}